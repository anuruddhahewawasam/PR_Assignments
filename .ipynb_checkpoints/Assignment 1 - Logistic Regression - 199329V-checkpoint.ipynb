{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"Data\\\\breast_cancer_wisconsin_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      "id                         569 non-null int64\n",
      "diagnosis                  569 non-null object\n",
      "radius_mean                569 non-null float64\n",
      "texture_mean               569 non-null float64\n",
      "perimeter_mean             569 non-null float64\n",
      "area_mean                  569 non-null float64\n",
      "smoothness_mean            569 non-null float64\n",
      "compactness_mean           569 non-null float64\n",
      "concavity_mean             569 non-null float64\n",
      "concave points_mean        569 non-null float64\n",
      "symmetry_mean              569 non-null float64\n",
      "fractal_dimension_mean     569 non-null float64\n",
      "radius_se                  569 non-null float64\n",
      "texture_se                 569 non-null float64\n",
      "perimeter_se               569 non-null float64\n",
      "area_se                    569 non-null float64\n",
      "smoothness_se              569 non-null float64\n",
      "compactness_se             569 non-null float64\n",
      "concavity_se               569 non-null float64\n",
      "concave points_se          569 non-null float64\n",
      "symmetry_se                569 non-null float64\n",
      "fractal_dimension_se       569 non-null float64\n",
      "radius_worst               569 non-null float64\n",
      "texture_worst              569 non-null float64\n",
      "perimeter_worst            569 non-null float64\n",
      "area_worst                 569 non-null float64\n",
      "smoothness_worst           569 non-null float64\n",
      "compactness_worst          569 non-null float64\n",
      "concavity_worst            569 non-null float64\n",
      "concave points_worst       569 non-null float64\n",
      "symmetry_worst             569 non-null float64\n",
      "fractal_dimension_worst    569 non-null float64\n",
      "Unnamed: 32                0 non-null float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the 'id' column from the dataset\n",
    "df=df.drop(\"id\",1)\n",
    "df=df.drop(\"Unnamed: 32\",1)\n",
    "    \n",
    "#Mapping M to 1 and B to 0 in the output Label DataFrame\n",
    "df['diagnosis']=df['diagnosis'].map({'M':1,'B':0})\n",
    "    \n",
    "    \n",
    "#Split Data into training and test (65% and 34%)\n",
    "train, test = train_test_split(df, test_size = 0.34, random_state=1)\n",
    "\n",
    "#Training Data\n",
    "train_x=train.loc[:,'radius_mean' : 'fractal_dimension_worst']\n",
    "train_y=train.loc[:,['diagnosis']]\n",
    "    \n",
    "#Testing Data\n",
    "test_x=test.loc[:,'radius_mean' : 'fractal_dimension_worst']\n",
    "test_y=test.loc[:,['diagnosis']]\n",
    "    \n",
    "#Converting Traing and Test Data to numpy array\n",
    "train_x=np.asarray(train_x)\n",
    "train_y=np.asarray(train_y)\n",
    "test_x=np.asarray(test_x)\n",
    "test_y=np.asarray(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to initialize the weights and bias\n",
    "def initialize(m):\n",
    "    \n",
    "    w = np.zeros((m,1))\n",
    "    b = 0\n",
    "    \n",
    "    return w , b\n",
    "    \n",
    "#Function to calculate sigmoid of x    \n",
    "def sigmoid(X):\n",
    "    return 1/(1 + np.exp(- X))    \n",
    "\n",
    "\n",
    "#Function for doing forward and back propogation\n",
    "def propogate(X, Y, w, b):\n",
    "    \n",
    "    m = X.shape[1] #Number of training examples\n",
    "\n",
    "    #Forward Propogation, calculating the cost\n",
    "    Z = np.dot(w.T, X) + b;    \n",
    "    A = sigmoid(Z)\n",
    "    cost= -(1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n",
    "    \n",
    "    #Back Propogation , calculating the gradients\n",
    "    dw = (1/m)* np.dot(X, (A-Y).T)\n",
    "    db = (1/m)* np.sum(A-Y)\n",
    "    \n",
    "    grads= {\"dw\" : dw, \"db\" : db}\n",
    "    \n",
    "    return grads, cost\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#Function for performing Grdient Descent\n",
    "def optimize(X, Y, w, b, num_of_iterations, alpha):\n",
    "    \n",
    "    costs=[] \n",
    "    \n",
    "    for i in range(num_of_iterations):\n",
    " \n",
    "        grads, cost = propogate(X, Y, w, b)\n",
    "        \n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        w = w - alpha * dw\n",
    "        b = b - alpha * db\n",
    "        \n",
    "        #Storing the cost at interval of every 10 iterations\n",
    "        if i% 10 == 0:\n",
    "            costs.append(cost)\n",
    "            print(\"cost after %i iteration : %f\" % (i, cost))\n",
    "            \n",
    "            \n",
    "    parameters = {\"w\":w, \"b\":b}\n",
    "    grads = {\"dw\":dw, \"db\":db}\n",
    "    \n",
    "    \n",
    "    return parameters, grads, costs\n",
    "\n",
    "\n",
    "#Function for doing the predictions on the data set (mapping probabilities to 0 or 1)\n",
    "def predict(X, w, b):\n",
    "    \n",
    "    m = X.shape[1] #Number of training examples\n",
    "    \n",
    "    y_prediction =  np.zeros((1,m))\n",
    "    \n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    A=sigmoid(np.dot(w.T, X)+b)\n",
    "    \n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        if(A[0,i]<0.5):\n",
    "            y_prediction[0,i]=0\n",
    "        else:\n",
    "            y_prediction[0,i]=1\n",
    "            \n",
    "    \n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calculating the Logistic Regression Model\n",
    "def model(Xtrain, Ytrain, num_of_iterations, alpha):\n",
    "    \n",
    "    dim = Xtrain.shape[0] #Number of features\n",
    "    \n",
    "    w,b = initialize(dim)\n",
    "    \n",
    "    parameters, grads, costs = optimize(Xtrain, Ytrain, w, b, num_of_iterations, alpha) \n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "        \n",
    "    \n",
    "    d={\"w\":w, \"b\":b, \"costs\": costs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0 iteration : 0.693147\n",
      "cost after 10 iteration : 0.667932\n",
      "cost after 20 iteration : 0.661586\n",
      "cost after 30 iteration : 0.655525\n",
      "cost after 40 iteration : 0.649726\n",
      "cost after 50 iteration : 0.644170\n",
      "cost after 60 iteration : 0.638839\n",
      "cost after 70 iteration : 0.633717\n",
      "cost after 80 iteration : 0.628790\n",
      "cost after 90 iteration : 0.624044\n",
      "cost after 100 iteration : 0.619467\n",
      "cost after 110 iteration : 0.615048\n",
      "cost after 120 iteration : 0.610776\n",
      "cost after 130 iteration : 0.606644\n",
      "cost after 140 iteration : 0.602642\n",
      "cost after 150 iteration : 0.598763\n",
      "cost after 160 iteration : 0.595000\n",
      "cost after 170 iteration : 0.591346\n",
      "cost after 180 iteration : 0.587796\n",
      "cost after 190 iteration : 0.584344\n",
      "cost after 200 iteration : 0.580985\n",
      "cost after 210 iteration : 0.577714\n",
      "cost after 220 iteration : 0.574528\n",
      "cost after 230 iteration : 0.571422\n",
      "cost after 240 iteration : 0.568392\n",
      "cost after 250 iteration : 0.565436\n",
      "cost after 260 iteration : 0.562549\n",
      "cost after 270 iteration : 0.559729\n",
      "cost after 280 iteration : 0.556973\n",
      "cost after 290 iteration : 0.554279\n",
      "cost after 300 iteration : 0.551643\n",
      "cost after 310 iteration : 0.549063\n",
      "cost after 320 iteration : 0.546538\n",
      "cost after 330 iteration : 0.544065\n",
      "cost after 340 iteration : 0.541643\n",
      "cost after 350 iteration : 0.539269\n",
      "cost after 360 iteration : 0.536941\n",
      "cost after 370 iteration : 0.534659\n",
      "cost after 380 iteration : 0.532419\n",
      "cost after 390 iteration : 0.530222\n",
      "cost after 400 iteration : 0.528066\n",
      "cost after 410 iteration : 0.525948\n",
      "cost after 420 iteration : 0.523869\n",
      "cost after 430 iteration : 0.521826\n",
      "cost after 440 iteration : 0.519819\n",
      "cost after 450 iteration : 0.517846\n",
      "cost after 460 iteration : 0.515907\n",
      "cost after 470 iteration : 0.514000\n",
      "cost after 480 iteration : 0.512124\n",
      "cost after 490 iteration : 0.510279\n",
      "cost after 500 iteration : 0.508463\n",
      "cost after 510 iteration : 0.506676\n",
      "cost after 520 iteration : 0.504917\n",
      "cost after 530 iteration : 0.503186\n",
      "cost after 540 iteration : 0.501480\n",
      "cost after 550 iteration : 0.499801\n",
      "cost after 560 iteration : 0.498146\n",
      "cost after 570 iteration : 0.496516\n",
      "cost after 580 iteration : 0.494909\n",
      "cost after 590 iteration : 0.493325\n",
      "cost after 600 iteration : 0.491764\n",
      "cost after 610 iteration : 0.490225\n",
      "cost after 620 iteration : 0.488708\n",
      "cost after 630 iteration : 0.487211\n",
      "cost after 640 iteration : 0.485734\n",
      "cost after 650 iteration : 0.484277\n",
      "cost after 660 iteration : 0.482840\n",
      "cost after 670 iteration : 0.481422\n",
      "cost after 680 iteration : 0.480022\n",
      "cost after 690 iteration : 0.478640\n",
      "cost after 700 iteration : 0.477276\n",
      "cost after 710 iteration : 0.475929\n",
      "cost after 720 iteration : 0.474599\n",
      "cost after 730 iteration : 0.473285\n",
      "cost after 740 iteration : 0.471988\n",
      "cost after 750 iteration : 0.470706\n",
      "cost after 760 iteration : 0.469439\n",
      "cost after 770 iteration : 0.468188\n",
      "cost after 780 iteration : 0.466952\n",
      "cost after 790 iteration : 0.465730\n",
      "cost after 800 iteration : 0.464522\n",
      "cost after 810 iteration : 0.463328\n",
      "cost after 820 iteration : 0.462147\n",
      "cost after 830 iteration : 0.460980\n",
      "cost after 840 iteration : 0.459826\n",
      "cost after 850 iteration : 0.458685\n",
      "cost after 860 iteration : 0.457556\n",
      "cost after 870 iteration : 0.456439\n",
      "cost after 880 iteration : 0.455335\n",
      "cost after 890 iteration : 0.454242\n",
      "cost after 900 iteration : 0.453161\n",
      "cost after 910 iteration : 0.452092\n",
      "cost after 920 iteration : 0.451033\n",
      "cost after 930 iteration : 0.449986\n",
      "cost after 940 iteration : 0.448949\n",
      "cost after 950 iteration : 0.447923\n",
      "cost after 960 iteration : 0.446907\n",
      "cost after 970 iteration : 0.445901\n",
      "cost after 980 iteration : 0.444905\n",
      "cost after 990 iteration : 0.443920\n",
      "cost after 1000 iteration : 0.442943\n",
      "cost after 1010 iteration : 0.441977\n",
      "cost after 1020 iteration : 0.441019\n",
      "cost after 1030 iteration : 0.440071\n",
      "cost after 1040 iteration : 0.439132\n",
      "cost after 1050 iteration : 0.438201\n",
      "cost after 1060 iteration : 0.437279\n",
      "cost after 1070 iteration : 0.436366\n",
      "cost after 1080 iteration : 0.435462\n",
      "cost after 1090 iteration : 0.434565\n",
      "cost after 1100 iteration : 0.433677\n",
      "cost after 1110 iteration : 0.432796\n",
      "cost after 1120 iteration : 0.431924\n",
      "cost after 1130 iteration : 0.431059\n",
      "cost after 1140 iteration : 0.430202\n",
      "cost after 1150 iteration : 0.429352\n",
      "cost after 1160 iteration : 0.428510\n",
      "cost after 1170 iteration : 0.427675\n",
      "cost after 1180 iteration : 0.426848\n",
      "cost after 1190 iteration : 0.426027\n",
      "cost after 1200 iteration : 0.425213\n",
      "cost after 1210 iteration : 0.424406\n",
      "cost after 1220 iteration : 0.423606\n",
      "cost after 1230 iteration : 0.422813\n",
      "cost after 1240 iteration : 0.422026\n",
      "cost after 1250 iteration : 0.421245\n",
      "cost after 1260 iteration : 0.420471\n",
      "cost after 1270 iteration : 0.419703\n",
      "cost after 1280 iteration : 0.418942\n",
      "cost after 1290 iteration : 0.418186\n",
      "cost after 1300 iteration : 0.417436\n",
      "cost after 1310 iteration : 0.416693\n",
      "cost after 1320 iteration : 0.415955\n",
      "cost after 1330 iteration : 0.415223\n",
      "cost after 1340 iteration : 0.414496\n",
      "cost after 1350 iteration : 0.413775\n",
      "cost after 1360 iteration : 0.413060\n",
      "cost after 1370 iteration : 0.412350\n",
      "cost after 1380 iteration : 0.411646\n",
      "cost after 1390 iteration : 0.410947\n",
      "cost after 1400 iteration : 0.410253\n",
      "cost after 1410 iteration : 0.409564\n",
      "cost after 1420 iteration : 0.408880\n",
      "cost after 1430 iteration : 0.408201\n",
      "cost after 1440 iteration : 0.407527\n",
      "cost after 1450 iteration : 0.406859\n",
      "cost after 1460 iteration : 0.406195\n",
      "cost after 1470 iteration : 0.405535\n",
      "cost after 1480 iteration : 0.404881\n",
      "cost after 1490 iteration : 0.404231\n",
      "cost after 1500 iteration : 0.403586\n",
      "cost after 1510 iteration : 0.402945\n",
      "cost after 1520 iteration : 0.402309\n",
      "cost after 1530 iteration : 0.401677\n",
      "cost after 1540 iteration : 0.401049\n",
      "cost after 1550 iteration : 0.400426\n",
      "cost after 1560 iteration : 0.399807\n",
      "cost after 1570 iteration : 0.399192\n",
      "cost after 1580 iteration : 0.398582\n",
      "cost after 1590 iteration : 0.397976\n",
      "cost after 1600 iteration : 0.397373\n",
      "cost after 1610 iteration : 0.396775\n",
      "cost after 1620 iteration : 0.396181\n",
      "cost after 1630 iteration : 0.395590\n",
      "cost after 1640 iteration : 0.395004\n",
      "cost after 1650 iteration : 0.394421\n",
      "cost after 1660 iteration : 0.393842\n",
      "cost after 1670 iteration : 0.393267\n",
      "cost after 1680 iteration : 0.392695\n",
      "cost after 1690 iteration : 0.392128\n",
      "cost after 1700 iteration : 0.391563\n",
      "cost after 1710 iteration : 0.391003\n",
      "cost after 1720 iteration : 0.390446\n",
      "cost after 1730 iteration : 0.389892\n",
      "cost after 1740 iteration : 0.389342\n",
      "cost after 1750 iteration : 0.388796\n",
      "cost after 1760 iteration : 0.388252\n",
      "cost after 1770 iteration : 0.387712\n",
      "cost after 1780 iteration : 0.387176\n",
      "cost after 1790 iteration : 0.386643\n",
      "cost after 1800 iteration : 0.386113\n",
      "cost after 1810 iteration : 0.385586\n",
      "cost after 1820 iteration : 0.385062\n",
      "cost after 1830 iteration : 0.384542\n",
      "cost after 1840 iteration : 0.384024\n",
      "cost after 1850 iteration : 0.383510\n",
      "cost after 1860 iteration : 0.382999\n",
      "cost after 1870 iteration : 0.382491\n",
      "cost after 1880 iteration : 0.381986\n",
      "cost after 1890 iteration : 0.381483\n",
      "cost after 1900 iteration : 0.380984\n",
      "cost after 1910 iteration : 0.380488\n",
      "cost after 1920 iteration : 0.379994\n",
      "cost after 1930 iteration : 0.379504\n",
      "cost after 1940 iteration : 0.379016\n",
      "cost after 1950 iteration : 0.378531\n",
      "cost after 1960 iteration : 0.378049\n",
      "cost after 1970 iteration : 0.377569\n",
      "cost after 1980 iteration : 0.377092\n",
      "cost after 1990 iteration : 0.376618\n",
      "cost after 2000 iteration : 0.376147\n",
      "cost after 2010 iteration : 0.375678\n",
      "cost after 2020 iteration : 0.375211\n",
      "cost after 2030 iteration : 0.374748\n",
      "cost after 2040 iteration : 0.374287\n",
      "cost after 2050 iteration : 0.373828\n",
      "cost after 2060 iteration : 0.373372\n",
      "cost after 2070 iteration : 0.372919\n",
      "cost after 2080 iteration : 0.372468\n",
      "cost after 2090 iteration : 0.372019\n",
      "cost after 2100 iteration : 0.371573\n",
      "cost after 2110 iteration : 0.371129\n",
      "cost after 2120 iteration : 0.370688\n",
      "cost after 2130 iteration : 0.370249\n",
      "cost after 2140 iteration : 0.369812\n",
      "cost after 2150 iteration : 0.369377\n",
      "cost after 2160 iteration : 0.368945\n",
      "cost after 2170 iteration : 0.368516\n",
      "cost after 2180 iteration : 0.368088\n",
      "cost after 2190 iteration : 0.367663\n",
      "cost after 2200 iteration : 0.367240\n",
      "cost after 2210 iteration : 0.366819\n",
      "cost after 2220 iteration : 0.366400\n",
      "cost after 2230 iteration : 0.365983\n",
      "cost after 2240 iteration : 0.365569\n",
      "cost after 2250 iteration : 0.365157\n",
      "cost after 2260 iteration : 0.364747\n",
      "cost after 2270 iteration : 0.364339\n",
      "cost after 2280 iteration : 0.363933\n",
      "cost after 2290 iteration : 0.363529\n",
      "cost after 2300 iteration : 0.363127\n",
      "cost after 2310 iteration : 0.362727\n",
      "cost after 2320 iteration : 0.362329\n",
      "cost after 2330 iteration : 0.361933\n",
      "cost after 2340 iteration : 0.361539\n",
      "cost after 2350 iteration : 0.361147\n",
      "cost after 2360 iteration : 0.360757\n",
      "cost after 2370 iteration : 0.360369\n",
      "cost after 2380 iteration : 0.359983\n",
      "cost after 2390 iteration : 0.359599\n",
      "cost after 2400 iteration : 0.359216\n",
      "cost after 2410 iteration : 0.358836\n",
      "cost after 2420 iteration : 0.358457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 2430 iteration : 0.358080\n",
      "cost after 2440 iteration : 0.357705\n",
      "cost after 2450 iteration : 0.357332\n",
      "cost after 2460 iteration : 0.356960\n",
      "cost after 2470 iteration : 0.356591\n",
      "cost after 2480 iteration : 0.356223\n",
      "cost after 2490 iteration : 0.355857\n",
      "cost after 2500 iteration : 0.355492\n",
      "cost after 2510 iteration : 0.355130\n",
      "cost after 2520 iteration : 0.354769\n",
      "cost after 2530 iteration : 0.354410\n",
      "cost after 2540 iteration : 0.354052\n",
      "cost after 2550 iteration : 0.353696\n",
      "cost after 2560 iteration : 0.353342\n",
      "cost after 2570 iteration : 0.352990\n",
      "cost after 2580 iteration : 0.352639\n",
      "cost after 2590 iteration : 0.352289\n",
      "cost after 2600 iteration : 0.351942\n",
      "cost after 2610 iteration : 0.351596\n",
      "cost after 2620 iteration : 0.351251\n",
      "cost after 2630 iteration : 0.350908\n",
      "cost after 2640 iteration : 0.350567\n",
      "cost after 2650 iteration : 0.350227\n",
      "cost after 2660 iteration : 0.349889\n",
      "cost after 2670 iteration : 0.349552\n",
      "cost after 2680 iteration : 0.349217\n",
      "cost after 2690 iteration : 0.348884\n",
      "cost after 2700 iteration : 0.348552\n",
      "cost after 2710 iteration : 0.348221\n",
      "cost after 2720 iteration : 0.347892\n",
      "cost after 2730 iteration : 0.347564\n",
      "cost after 2740 iteration : 0.347238\n",
      "cost after 2750 iteration : 0.346913\n",
      "cost after 2760 iteration : 0.346590\n",
      "cost after 2770 iteration : 0.346268\n",
      "cost after 2780 iteration : 0.345948\n",
      "cost after 2790 iteration : 0.345628\n",
      "cost after 2800 iteration : 0.345311\n",
      "cost after 2810 iteration : 0.344995\n",
      "cost after 2820 iteration : 0.344680\n",
      "cost after 2830 iteration : 0.344366\n",
      "cost after 2840 iteration : 0.344054\n",
      "cost after 2850 iteration : 0.343743\n",
      "cost after 2860 iteration : 0.343434\n",
      "cost after 2870 iteration : 0.343126\n",
      "cost after 2880 iteration : 0.342819\n",
      "cost after 2890 iteration : 0.342514\n",
      "cost after 2900 iteration : 0.342210\n",
      "cost after 2910 iteration : 0.341907\n",
      "cost after 2920 iteration : 0.341605\n",
      "cost after 2930 iteration : 0.341305\n",
      "cost after 2940 iteration : 0.341006\n",
      "cost after 2950 iteration : 0.340709\n",
      "cost after 2960 iteration : 0.340412\n",
      "cost after 2970 iteration : 0.340117\n",
      "cost after 2980 iteration : 0.339823\n",
      "cost after 2990 iteration : 0.339531\n",
      "cost after 3000 iteration : 0.339239\n",
      "cost after 3010 iteration : 0.338949\n",
      "cost after 3020 iteration : 0.338660\n",
      "cost after 3030 iteration : 0.338372\n",
      "cost after 3040 iteration : 0.338086\n",
      "cost after 3050 iteration : 0.337800\n",
      "cost after 3060 iteration : 0.337516\n",
      "cost after 3070 iteration : 0.337233\n",
      "cost after 3080 iteration : 0.336951\n",
      "cost after 3090 iteration : 0.336671\n",
      "cost after 3100 iteration : 0.336391\n",
      "cost after 3110 iteration : 0.336113\n",
      "cost after 3120 iteration : 0.335836\n",
      "cost after 3130 iteration : 0.335559\n",
      "cost after 3140 iteration : 0.335284\n",
      "cost after 3150 iteration : 0.335011\n",
      "cost after 3160 iteration : 0.334738\n",
      "cost after 3170 iteration : 0.334466\n",
      "cost after 3180 iteration : 0.334196\n",
      "cost after 3190 iteration : 0.333926\n",
      "cost after 3200 iteration : 0.333658\n",
      "cost after 3210 iteration : 0.333391\n",
      "cost after 3220 iteration : 0.333125\n",
      "cost after 3230 iteration : 0.332859\n",
      "cost after 3240 iteration : 0.332595\n",
      "cost after 3250 iteration : 0.332332\n",
      "cost after 3260 iteration : 0.332070\n",
      "cost after 3270 iteration : 0.331810\n",
      "cost after 3280 iteration : 0.331550\n",
      "cost after 3290 iteration : 0.331291\n",
      "cost after 3300 iteration : 0.331033\n",
      "cost after 3310 iteration : 0.330776\n",
      "cost after 3320 iteration : 0.330521\n",
      "cost after 3330 iteration : 0.330266\n",
      "cost after 3340 iteration : 0.330012\n",
      "cost after 3350 iteration : 0.329759\n",
      "cost after 3360 iteration : 0.329508\n",
      "cost after 3370 iteration : 0.329257\n",
      "cost after 3380 iteration : 0.329007\n",
      "cost after 3390 iteration : 0.328758\n",
      "cost after 3400 iteration : 0.328510\n",
      "cost after 3410 iteration : 0.328263\n",
      "cost after 3420 iteration : 0.328017\n",
      "cost after 3430 iteration : 0.327772\n",
      "cost after 3440 iteration : 0.327528\n",
      "cost after 3450 iteration : 0.327285\n",
      "cost after 3460 iteration : 0.327043\n",
      "cost after 3470 iteration : 0.326802\n",
      "cost after 3480 iteration : 0.326561\n",
      "cost after 3490 iteration : 0.326322\n",
      "cost after 3500 iteration : 0.326083\n",
      "cost after 3510 iteration : 0.325846\n",
      "cost after 3520 iteration : 0.325609\n",
      "cost after 3530 iteration : 0.325373\n",
      "cost after 3540 iteration : 0.325139\n",
      "cost after 3550 iteration : 0.324905\n",
      "cost after 3560 iteration : 0.324671\n",
      "cost after 3570 iteration : 0.324439\n",
      "cost after 3580 iteration : 0.324208\n",
      "cost after 3590 iteration : 0.323977\n",
      "cost after 3600 iteration : 0.323748\n",
      "cost after 3610 iteration : 0.323519\n",
      "cost after 3620 iteration : 0.323291\n",
      "cost after 3630 iteration : 0.323064\n",
      "cost after 3640 iteration : 0.322838\n",
      "cost after 3650 iteration : 0.322612\n",
      "cost after 3660 iteration : 0.322388\n",
      "cost after 3670 iteration : 0.322164\n",
      "cost after 3680 iteration : 0.321941\n",
      "cost after 3690 iteration : 0.321719\n",
      "cost after 3700 iteration : 0.321498\n",
      "cost after 3710 iteration : 0.321277\n",
      "cost after 3720 iteration : 0.321058\n",
      "cost after 3730 iteration : 0.320839\n",
      "cost after 3740 iteration : 0.320621\n",
      "cost after 3750 iteration : 0.320404\n",
      "cost after 3760 iteration : 0.320187\n",
      "cost after 3770 iteration : 0.319972\n",
      "cost after 3780 iteration : 0.319757\n",
      "cost after 3790 iteration : 0.319543\n",
      "cost after 3800 iteration : 0.319329\n",
      "cost after 3810 iteration : 0.319117\n",
      "cost after 3820 iteration : 0.318905\n",
      "cost after 3830 iteration : 0.318694\n",
      "cost after 3840 iteration : 0.318484\n",
      "cost after 3850 iteration : 0.318274\n",
      "cost after 3860 iteration : 0.318066\n",
      "cost after 3870 iteration : 0.317858\n",
      "cost after 3880 iteration : 0.317651\n",
      "cost after 3890 iteration : 0.317444\n",
      "cost after 3900 iteration : 0.317238\n",
      "cost after 3910 iteration : 0.317033\n",
      "cost after 3920 iteration : 0.316829\n",
      "cost after 3930 iteration : 0.316625\n",
      "cost after 3940 iteration : 0.316422\n",
      "cost after 3950 iteration : 0.316220\n",
      "cost after 3960 iteration : 0.316019\n",
      "cost after 3970 iteration : 0.315818\n",
      "cost after 3980 iteration : 0.315618\n",
      "cost after 3990 iteration : 0.315419\n",
      "cost after 4000 iteration : 0.315220\n",
      "cost after 4010 iteration : 0.315022\n",
      "cost after 4020 iteration : 0.314825\n",
      "cost after 4030 iteration : 0.314628\n",
      "cost after 4040 iteration : 0.314433\n",
      "cost after 4050 iteration : 0.314237\n",
      "cost after 4060 iteration : 0.314043\n",
      "cost after 4070 iteration : 0.313849\n",
      "cost after 4080 iteration : 0.313656\n",
      "cost after 4090 iteration : 0.313463\n",
      "cost after 4100 iteration : 0.313272\n",
      "cost after 4110 iteration : 0.313080\n",
      "cost after 4120 iteration : 0.312890\n",
      "cost after 4130 iteration : 0.312700\n",
      "cost after 4140 iteration : 0.312511\n",
      "cost after 4150 iteration : 0.312322\n",
      "cost after 4160 iteration : 0.312134\n",
      "cost after 4170 iteration : 0.311947\n",
      "cost after 4180 iteration : 0.311760\n",
      "cost after 4190 iteration : 0.311574\n",
      "cost after 4200 iteration : 0.311389\n",
      "cost after 4210 iteration : 0.311204\n",
      "cost after 4220 iteration : 0.311020\n",
      "cost after 4230 iteration : 0.310836\n",
      "cost after 4240 iteration : 0.310654\n",
      "cost after 4250 iteration : 0.310471\n",
      "cost after 4260 iteration : 0.310290\n",
      "cost after 4270 iteration : 0.310109\n",
      "cost after 4280 iteration : 0.309928\n",
      "cost after 4290 iteration : 0.309748\n",
      "cost after 4300 iteration : 0.309569\n",
      "cost after 4310 iteration : 0.309390\n",
      "cost after 4320 iteration : 0.309212\n",
      "cost after 4330 iteration : 0.309035\n",
      "cost after 4340 iteration : 0.308858\n",
      "cost after 4350 iteration : 0.308682\n",
      "cost after 4360 iteration : 0.308506\n",
      "cost after 4370 iteration : 0.308331\n",
      "cost after 4380 iteration : 0.308156\n",
      "cost after 4390 iteration : 0.307982\n",
      "cost after 4400 iteration : 0.307809\n",
      "cost after 4410 iteration : 0.307636\n",
      "cost after 4420 iteration : 0.307464\n",
      "cost after 4430 iteration : 0.307292\n",
      "cost after 4440 iteration : 0.307121\n",
      "cost after 4450 iteration : 0.306950\n",
      "cost after 4460 iteration : 0.306780\n",
      "cost after 4470 iteration : 0.306610\n",
      "cost after 4480 iteration : 0.306442\n",
      "cost after 4490 iteration : 0.306273\n",
      "cost after 4500 iteration : 0.306105\n",
      "cost after 4510 iteration : 0.305938\n",
      "cost after 4520 iteration : 0.305771\n",
      "cost after 4530 iteration : 0.305605\n",
      "cost after 4540 iteration : 0.305439\n",
      "cost after 4550 iteration : 0.305274\n",
      "cost after 4560 iteration : 0.305109\n",
      "cost after 4570 iteration : 0.304945\n",
      "cost after 4580 iteration : 0.304782\n",
      "cost after 4590 iteration : 0.304618\n",
      "cost after 4600 iteration : 0.304456\n",
      "cost after 4610 iteration : 0.304294\n",
      "cost after 4620 iteration : 0.304132\n",
      "cost after 4630 iteration : 0.303971\n",
      "cost after 4640 iteration : 0.303811\n",
      "cost after 4650 iteration : 0.303651\n",
      "cost after 4660 iteration : 0.303491\n",
      "cost after 4670 iteration : 0.303332\n",
      "cost after 4680 iteration : 0.303174\n",
      "cost after 4690 iteration : 0.303016\n",
      "cost after 4700 iteration : 0.302858\n",
      "cost after 4710 iteration : 0.302701\n",
      "cost after 4720 iteration : 0.302545\n",
      "cost after 4730 iteration : 0.302388\n",
      "cost after 4740 iteration : 0.302233\n",
      "cost after 4750 iteration : 0.302078\n",
      "cost after 4760 iteration : 0.301923\n",
      "cost after 4770 iteration : 0.301769\n",
      "cost after 4780 iteration : 0.301615\n",
      "cost after 4790 iteration : 0.301462\n",
      "cost after 4800 iteration : 0.301309\n",
      "cost after 4810 iteration : 0.301157\n",
      "cost after 4820 iteration : 0.301005\n",
      "cost after 4830 iteration : 0.300854\n",
      "cost after 4840 iteration : 0.300703\n",
      "cost after 4850 iteration : 0.300553\n",
      "cost after 4860 iteration : 0.300403\n",
      "cost after 4870 iteration : 0.300253\n",
      "cost after 4880 iteration : 0.300104\n",
      "cost after 4890 iteration : 0.299956\n",
      "cost after 4900 iteration : 0.299808\n",
      "cost after 4910 iteration : 0.299660\n",
      "cost after 4920 iteration : 0.299513\n",
      "cost after 4930 iteration : 0.299366\n",
      "cost after 4940 iteration : 0.299220\n",
      "cost after 4950 iteration : 0.299074\n",
      "cost after 4960 iteration : 0.298928\n",
      "cost after 4970 iteration : 0.298783\n",
      "cost after 4980 iteration : 0.298639\n",
      "cost after 4990 iteration : 0.298495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train accuracy: 91.2 %\n",
      "\n",
      "Test accuracy: 92.26804123711341 %\n"
     ]
    }
   ],
   "source": [
    "def accuracy():           \n",
    "    #Calling the model function to train a Logistic Regression Model on Training Data\n",
    "    d= model(train_x.T, train_y.T, num_of_iterations=5000, alpha=0.000001)\n",
    "    \n",
    "        \n",
    "    costs=d[\"costs\"]\n",
    "    w=d[\"w\"]\n",
    "    b=d[\"b\"]\n",
    "    \n",
    "    \n",
    "    #Now, calculating the accuracy on Training and Test Data\n",
    "    Y_prediction_train = predict(train_x.T, w, b)\n",
    "    Y_prediction_test = predict(test_x.T, w, b)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"\\nTrain accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - train_y.T)) * 100))\n",
    "    \n",
    "    print(\"\\nTest accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - test_y.T)) * 100))\n",
    "\n",
    "#Calling the accuracy function to output the results\n",
    "accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
